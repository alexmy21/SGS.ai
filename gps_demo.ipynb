{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLLSETS_PATH:  /home/alexmy/SGS/SGS.ai/sgs_core/HllSets/src/HllSets.jl\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "path = os.getenv(\"HLLSETS_PATH\")\n",
    "print(\"HLLSETS_PATH: \", path)\n",
    "\n",
    "# Import the module from sgs_core\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import hdf5\n",
    "\n",
    "# Add the sgs_core directory to the Python path\n",
    "sys.path.append(str(Path.cwd() / \"sgs_core\"))\n",
    "# sys.path.index(os.path.abspath(path))\n",
    "\n",
    "# Import the meta_algebra module\n",
    "import meta_algebra\n",
    "# import u_controller \n",
    "\n",
    "# Test a method from meta_algebra\n",
    "hll = meta_algebra.HllSet()\n",
    "result = hll.count()  # Replace 'some_method' with the actual method name\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_triangulation.ipynb\n",
    "\n",
    "# Cell 1: Setup and Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import random\n",
    "# from hll_wrapper import HLLSetWrapper, TokenGenerator\n",
    "from triangulation import SemanticTriangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üöÄ Multiple Triangulation Methods Demo\")\n",
    "print(\"======================================\")\n",
    "\n",
    "# Cell 2: Initialize Components\n",
    "print(\"Initializing HLLSet wrapper and token generator...\")\n",
    "\n",
    "hll_wrapper = HLLSetWrapper(p=10)\n",
    "token_gen = TokenGenerator(vocab_size=500)\n",
    "triangulation = SemanticTriangulation(hll_wrapper, num_seeds=6)\n",
    "\n",
    "# Get categorized tokens for meaningful demonstration\n",
    "categories = token_gen.get_token_categories()\n",
    "all_tokens = [token for category_tokens in categories.values() for token in category_tokens]\n",
    "\n",
    "print(f\"Generated {len(all_tokens)} tokens across {len(categories)} categories\")\n",
    "print(\"Categories:\", list(categories.keys()))\n",
    "\n",
    "# Cell 3: Create Test Scenario\n",
    "print(\"\\nüéØ Creating Test Scenario\")\n",
    "print(\"=========================\")\n",
    "\n",
    "# Select a target set of tokens (what we're trying to recover)\n",
    "target_tokens = random.sample(all_tokens, 20)\n",
    "print(f\"Target tokens ({len(target_tokens)}): {target_tokens[:5]}...\")\n",
    "\n",
    "# Create candidate pool (includes target tokens + noise)\n",
    "candidate_pool = target_tokens + random.sample(\n",
    "    [t for t in all_tokens if t not in target_tokens], 80\n",
    ")\n",
    "random.shuffle(candidate_pool)\n",
    "\n",
    "print(f\"Candidate pool: {len(candidate_pool)} tokens\")\n",
    "print(f\"Signal-to-noise ratio: {len(target_tokens)}:{len(candidate_pool)-len(target_tokens)}\")\n",
    "\n",
    "# Cell 4: Generate Multi-Seed Observations\n",
    "print(\"\\nüì° Generating Multi-Seed Observations\")\n",
    "print(\"====================================\")\n",
    "\n",
    "observations = triangulation.create_multi_seed_observations(target_tokens)\n",
    "print(f\"Created observations using {len(observations)} seeds\")\n",
    "\n",
    "# Show observation characteristics\n",
    "for i, (seed, obs) in enumerate(observations.items()):\n",
    "    print(f\"Seed {i+1}: ID={obs['id'][:8]}..., Cardinality={obs['cardinality']}\")\n",
    "\n",
    "# Cell 5: Basic Triangulation\n",
    "print(\"\\nüîç Basic Triangulation (Intersection Method)\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "basic_result = triangulation.basic_triangulation(observations, candidate_pool)\n",
    "print(f\"Basic triangulation found {len(basic_result)} tokens\")\n",
    "\n",
    "# Calculate precision and recall\n",
    "true_positives = len(basic_result.intersection(target_tokens))\n",
    "false_positives = len(basic_result - set(target_tokens))\n",
    "false_negatives = len(set(target_tokens) - basic_result)\n",
    "\n",
    "precision = true_positives / len(basic_result) if basic_result else 0\n",
    "recall = true_positives / len(target_tokens) if target_tokens else 0\n",
    "\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall: {recall:.2%}\")\n",
    "print(f\"F1-score: {2 * precision * recall / (precision + recall):.2%}\" if (precision + recall) > 0 else \"N/A\")\n",
    "\n",
    "# Cell 6: Weighted Triangulation\n",
    "print(\"\\n‚öñÔ∏è Weighted Triangulation\")\n",
    "print(\"=======================\")\n",
    "\n",
    "# Assign higher weights to first few seeds (simulating better \"satellites\")\n",
    "seed_weights = {seed: 1.0 + 0.5 * i for i, seed in enumerate(observations.keys())}\n",
    "weighted_scores = triangulation.weighted_triangulation(observations, candidate_pool, seed_weights)\n",
    "\n",
    "# Get top candidates\n",
    "threshold = 0.7\n",
    "weighted_candidates = [token for token, score in weighted_scores.items() if score >= threshold]\n",
    "\n",
    "print(f\"Weighted triangulation found {len(weighted_candidates)} candidates (score ‚â• {threshold})\")\n",
    "\n",
    "# Calculate metrics for weighted approach\n",
    "weighted_precision = len(set(weighted_candidates) & set(target_tokens)) / len(weighted_candidates) if weighted_candidates else 0\n",
    "weighted_recall = len(set(weighted_candidates) & set(target_tokens)) / len(target_tokens)\n",
    "\n",
    "print(f\"Precision: {weighted_precision:.2%}\")\n",
    "print(f\"Recall: {weighted_recall:.2%}\")\n",
    "\n",
    "# Cell 7: Progressive Triangulation\n",
    "print(\"\\nüîÑ Progressive Triangulation\")\n",
    "print(\"===========================\")\n",
    "\n",
    "progressive_result = triangulation.progressive_triangulation(observations, candidate_pool)\n",
    "\n",
    "print(f\"Final result: {len(progressive_result['final_candidates'])} tokens\")\n",
    "print(f\"Seeds used: {len(progressive_result['seeds_used'])}\")\n",
    "print(f\"Convergence achieved at iteration {len(progressive_result['convergence_history'])}\")\n",
    "\n",
    "# Plot convergence\n",
    "history = progressive_result['convergence_history']\n",
    "iterations = [h['iteration'] for h in history]\n",
    "candidate_sizes = [h['candidate_size'] for h in history]\n",
    "confidences = [h['confidence'] for h in history]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(iterations, candidate_sizes, 'bo-', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Candidate Set Size')\n",
    "plt.title('Progressive Disambiguation')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(iterations, confidences, 'ro-', linewidth=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Confidence')\n",
    "plt.title('Confidence Progression')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 8: Bayesian Triangulation\n",
    "print(\"\\nüé≤ Bayesian Triangulation\")\n",
    "print(\"=======================\")\n",
    "\n",
    "# Create non-uniform priors (some tokens more likely)\n",
    "priors = {token: 0.5 + random.random() * 0.5 for token in candidate_pool}  # 0.5-1.0 range\n",
    "# Normalize\n",
    "total_prior = sum(priors.values())\n",
    "priors = {k: v/total_prior for k, v in priors.items()}\n",
    "\n",
    "bayesian_probs = triangulation.bayesian_triangulation(observations, candidate_pool, priors)\n",
    "\n",
    "# Get high probability tokens\n",
    "prob_threshold = 0.1\n",
    "high_prob_tokens = [token for token, prob in bayesian_probs.items() if prob >= prob_threshold]\n",
    "\n",
    "print(f\"Bayesian approach found {len(high_prob_tokens)} high-probability tokens (P ‚â• {prob_threshold})\")\n",
    "\n",
    "# Show top 5 tokens by probability\n",
    "top_tokens = sorted(bayesian_probs.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top 5 tokens by probability:\")\n",
    "for token, prob in top_tokens:\n",
    "    status = \"‚úì\" if token in target_tokens else \"‚úó\"\n",
    "    print(f\"  {status} {token}: {prob:.3f}\")\n",
    "\n",
    "# Cell 9: Robust Triangulation\n",
    "print(\"\\nüõ°Ô∏è Robust Triangulation (Outlier Detection)\")\n",
    "print(\"==========================================\")\n",
    "\n",
    "# Simulate a noisy observation by adding an outlier\n",
    "noisy_observations = observations.copy()\n",
    "outlier_seed = max(observations.keys()) + 1\n",
    "noisy_observations[outlier_seed] = hll_wrapper.create_set(random.sample(all_tokens, 15), seed=outlier_seed)\n",
    "\n",
    "robust_result = triangulation.robust_triangulation(noisy_observations, candidate_pool)\n",
    "\n",
    "print(f\"Robust method detected {len(robust_result['outlier_seeds'])} outlier seeds\")\n",
    "print(f\"Final tokens: {len(robust_result['tokens'])}\")\n",
    "print(f\"Method used: {robust_result['method']}\")\n",
    "\n",
    "# Compare with non-robust approach on noisy data\n",
    "non_robust_result = triangulation.basic_triangulation(noisy_observations, candidate_pool)\n",
    "print(f\"Non-robust approach found {len(non_robust_result)} tokens\")\n",
    "\n",
    "# Cell 10: Comparative Analysis\n",
    "print(\"\\nüìä Comparative Analysis\")\n",
    "print(\"======================\")\n",
    "\n",
    "methods = {\n",
    "    \"Basic\": basic_result,\n",
    "    \"Weighted\": set(weighted_candidates),\n",
    "    \"Progressive\": progressive_result['final_candidates'],\n",
    "    \"Bayesian\": set(high_prob_tokens),\n",
    "    \"Robust\": robust_result['tokens']\n",
    "}\n",
    "\n",
    "# Calculate metrics for each method\n",
    "comparison_data = []\n",
    "for method_name, result_tokens in methods.items():\n",
    "    true_pos = len(result_tokens & set(target_tokens))\n",
    "    false_pos = len(result_tokens - set(target_tokens))\n",
    "    false_neg = len(set(target_tokens) - result_tokens)\n",
    "    \n",
    "    precision = true_pos / len(result_tokens) if result_tokens else 0\n",
    "    recall = true_pos / len(target_tokens) if target_tokens else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Method': method_name,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Tokens Found': len(result_tokens),\n",
    "        'True Positives': true_pos\n",
    "    })\n",
    "\n",
    "# Display comparison table\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "print(df_comparison.round(3))\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_pos = np.arange(len(methods))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x_pos - width, df_comparison['Precision'], width, label='Precision', alpha=0.8)\n",
    "plt.bar(x_pos, df_comparison['Recall'], width, label='Recall', alpha=0.8)\n",
    "plt.bar(x_pos + width, df_comparison['F1-Score'], width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Triangulation Method Comparison')\n",
    "plt.xticks(x_pos, df_comparison['Method'])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 11: Military Triangulation Analogy\n",
    "print(\"\\nüéñÔ∏è Military Triangulation Analogy\")\n",
    "print(\"================================\")\n",
    "\n",
    "print(\"\"\"\n",
    "Geo-Location Concept ‚Üí Semantic Disambiguation\n",
    "------------------------------------------------\n",
    "Satellites           ‚Üí Hash seeds\n",
    "Signal measurements  ‚Üí Bit position observations  \n",
    "Triangulation        ‚Üí Set intersection\n",
    "Error ellipses       ‚Üí Candidate token sets\n",
    "GPS accuracy         ‚Üí Disambiguation precision\n",
    "Signal noise         ‚Üí Hash collisions\n",
    "\n",
    "Key Insight: Just as military GPS uses multiple satellites to \n",
    "triangulate position, we use multiple hash seeds to triangulate \n",
    "token identity through consensus across independent observations.\n",
    "\"\"\")\n",
    "\n",
    "# Demonstrate the \"semantic GPS\" concept\n",
    "print(\"Semantic GPS Demonstration:\")\n",
    "print(f\"‚Ä¢ Target tokens: {len(target_tokens)} 'emitters' to locate\")\n",
    "print(f\"‚Ä¢ Satellite seeds: {triangulation.num_seeds} independent observers\")  \n",
    "print(f\"‚Ä¢ Candidate area: {len(candidate_pool)} possible locations\")\n",
    "print(f\"‚Ä¢ Best precision: {df_comparison['Precision'].max():.1%}\")\n",
    "\n",
    "# Cell 12: Conclusion and Insights\n",
    "print(\"\\nüí° Key Insights\")\n",
    "print(\"==============\")\n",
    "\n",
    "print(\"\"\"\n",
    "1. **Multiple Seeds Essential**: Single hash seeds suffer from ambiguity, \n",
    "   but multiple seeds provide independent \"votes\" for disambiguation.\n",
    "\n",
    "2. **Progressive Refinement**: Adding seeds sequentially dramatically \n",
    "   reduces candidate set size, similar to GPS getting more satellite fixes.\n",
    "\n",
    "3. **Robustness Matters**: Real-world data has noise; robust methods \n",
    "   that detect and ignore outliers perform better.\n",
    "\n",
    "4. **Prior Knowledge Helps**: Bayesian methods leverage domain knowledge \n",
    "   to improve disambiguation when available.\n",
    "\n",
    "5. **Military Analogy Holds**: The triangulation concept from geolocation \n",
    "   directly applies to semantic disambiguation with remarkable similarity.\n",
    "\"\"\")\n",
    "\n",
    "print(\"üéØ Demo Complete! The multiple triangulation approach successfully\")\n",
    "print(\"transforms HLLSet ambiguity from a weakness into a powerful\")\n",
    "print(\"disambiguation mechanism through consensus across hash seeds.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
